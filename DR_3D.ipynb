{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1d0309e-1ff8-4bb4-bb23-4db80a4cd384",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rid of warning messages and CUDA loadings\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'\n",
    "\n",
    "# Import Packages\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import cm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "e46829b6-db3c-4c27-92a8-b78418ef3012",
   "metadata": {},
   "outputs": [],
   "source": [
    "Adam_Epochs = 100 # Number of epochs of Adam optimization\n",
    "q = 100 # Number of RK time steps (max 500)\n",
    "noise = 0.01 # number of standard deviations of output noise\n",
    "D=1.001875195652246475e-02\n",
    "r=5.011364221572875977e-01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ab131469-11a1-4c1b-bb44-99bd66911ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "path='data/Fisher_3D_Symmetric.mat'\n",
    "data=scipy.io.loadmat(path)\n",
    "t = data['t'].flatten()[:,None]\n",
    "x = data['x'][:,0].flatten()[:,None]\n",
    "y = data['x'][:,1].flatten()[:,None]\n",
    "z = data['x'][:,2].flatten()[:,None]\n",
    "Exact_u = np.abs(data['u'])\n",
    "#create a time/space grid\n",
    "X, T = np.meshgrid(x,t)\n",
    "Y, T = np.meshgrid(y,t)\n",
    "Z, T = np.meshgrid(z,t)\n",
    "#flatten X and T\n",
    "xt = np.hstack((X.flatten()[:,None], T.flatten()[:,None])) #flatten X and T\n",
    "yt = np.hstack((Y.flatten()[:,None], T.flatten()[:,None])) #flatten Y and T\n",
    "zt = np.hstack((Z.flatten()[:,None], T.flatten()[:,None])) #flatten Z and T\n",
    "xyzt = np.array([xt[:,0], yt[:,0], zt[:,0], xt[:,1]]).T # all (x,y,t) combos in an array (shape = (x*y*z*t, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "afeccee1-caf7-442e-bef4-ea7e75e85048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29791, 1)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "00897280-c3bd-41bc-9650-49392e755a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((3008891, 1), (3008891, 4))"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "u_star = np.array([])\n",
    "for i in range(len(t)):\n",
    "    u_star = np.append(u_star, Exact_u[:,:,:,i].T.flatten())\n",
    "u_star = u_star.reshape(len(u_star),1) # values of the solution corresponding to the inputs of xyt\n",
    "u_star.shape,xyzt.shape #31*31*31*101，data['x'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "81a7c4d0-f4eb-40e9-a991-7364b112b60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the model structure\n",
    "Inputs=tf.keras.Input(shape=(3,))#1D input\n",
    "layer_number=100\n",
    "Dense_1=tf.keras.layers.Dense(layer_number,activation='tanh',kernel_initializer=\"glorot_normal\")(Inputs)\n",
    "Dense_2=tf.keras.layers.Dense(layer_number,activation='tanh',kernel_initializer=\"glorot_normal\")(Dense_1)\n",
    "Dense_3=tf.keras.layers.Dense(layer_number,activation='tanh',kernel_initializer=\"glorot_normal\")(Dense_2)\n",
    "Dense_4=tf.keras.layers.Dense(layer_number,activation='tanh',kernel_initializer=\"glorot_normal\")(Dense_3)\n",
    "#output layer dimension is q\n",
    "Predictions=tf.keras.layers.Dense(q)(Dense_4)\n",
    "model_predict=tf.keras.Model(inputs=Inputs,outputs=Predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6bf1129c-3d0f-4c6e-a740-259180d96ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameter for initial condition\n",
    "IRK_weights=np.float32(np.loadtxt('data/Butcher_IRK100.txt',ndmin=2))\n",
    "IRK_times = IRK_weights[q**2+q:]\n",
    "IRK_weights = IRK_weights[:q**2+q].reshape((q+1,q))\n",
    "IRK_alpha = tf.constant(IRK_weights[:-1,:], dtype='float32')\n",
    "IRK_beta = tf.constant(IRK_weights[-1:,:], dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "43ee50ef-ed04-469f-bce1-665f31925063",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = xyzt[:int(np.cbrt(x.shape[0]))*int(np.cbrt(y.shape[0]))*int(np.cbrt(z.shape[0])),0:3]\n",
    "outputs_t0 = u_star[:int(np.cbrt(x.shape[0]))*int(np.cbrt(y.shape[0]))*int(np.cbrt(z.shape[0])),0] # Initial snapshot solution data\n",
    "outputs_t1 = u_star[-int(np.cbrt(x.shape[0]))*int(np.cbrt(y.shape[0]))*int(np.cbrt(z.shape[0])):,0] # Final snapshot solution data\n",
    "outputs_t0_noise = outputs_t0 + noise*np.std(outputs_t0)*np.random.randn(outputs_t0.shape[0]) # Initial snapshot solution data with noise\n",
    "outputs_t1_noise = outputs_t1 + noise*np.std(outputs_t1)*np.random.randn(outputs_t1.shape[0]) # Final snapshot solution data with noise\n",
    "dt=(t[-1]-t[0])[0]\n",
    "#normalize the data\n",
    "input_min=inputs.min(0)#minimum of each column\n",
    "input_max=inputs.max(0)#maximum of each column\n",
    "inputs_norm=2*(inputs-input_min)/(input_max-input_min)-1.0#normalize the inputs\n",
    "\n",
    "#define tf,variables for neural network\n",
    "inputs_var = tf.Variable(inputs_norm, name='inputs_var')\n",
    "dummy=tf.ones((inputs_var.shape[0],q),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "7ba92813-8829-4249-86bf-ea027fdcfff3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3008891, 4)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xyzt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "748f931f-08e8-436a-b494-81e7ced31f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "##define solution of the PDE\n",
    "class PDEsolution(tf.keras.layers.Layer):\n",
    "    def __init__(self, D_var, r_var):\n",
    "        super(PDEsolution, self).__init__()\n",
    "        self.D_var = tf.Variable(np.array([D_var]))\n",
    "        self.r_var = tf.Variable(np.array([r_var]))\n",
    "\n",
    "    def loss(self,pred):   \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "                tape.watch(inputs_var)\n",
    "                tape.watch(dummy)\n",
    "                k = model_predict(inputs_var)\n",
    "                g_U = tape.gradient(k, inputs_var, output_gradients=dummy) #U 关于x和y的导数\n",
    "                g_Ux = g_U[:,0] #U 关于x的导数\n",
    "                g_Uy = g_U[:,1]  #U 关于y的导数\n",
    "                g_Uz = g_U[:,2]  #U关于z的导数 \n",
    "                k_x = tape.gradient(g_Ux, dummy)\n",
    "                k_y = tape.gradient(g_Uy, dummy)\n",
    "                k_z = tape.gradient(g_Uz, dummy)\n",
    "                g_Ux_U=tape.gradient(k_x,inputs_var,output_gradients=dummy) #Ux 关于x和y的导数\n",
    "                g_Uy_U=tape.gradient(k_y,inputs_var,output_gradients=dummy) #Uy 关于x和y的导数\n",
    "                g_Uz_U=tape.gradient(k_z,inputs_var,output_gradients=dummy) #Uz 关于x和y的导数\n",
    "                g_UxUx = g_Ux_U[:,0] #Ux 关于x的导数\n",
    "                g_UyUy = g_Uy_U[:,1] #Uy 关于y的导数\n",
    "                g_UzUz = g_Uz_U[:,2] #Uz 关于z的导数\n",
    "                k_xx = tape.gradient(g_UxUx, dummy) #U 关于x的二阶导数\n",
    "                k_yy = tape.gradient(g_UyUy, dummy) #U 关于y的二阶导数\n",
    "                k_zz = tape.gradient(g_UzUz, dummy) #U 关于y的二阶导数\n",
    "        scale_x = 2/(input_max[0]-input_min[0]) # Define scale factors and scale derivatives\n",
    "        scale_y = 2/(input_max[1]-input_min[1]) # Define scale factors and scale derivatives,因为前面进行了归一化\n",
    "        scale_z = 2/(input_max[2]-input_min[2])\n",
    "        k_x = k_x*scale_x\n",
    "        k_y = k_y*scale_y\n",
    "        k_z = k_z*scale_z\n",
    "        k_xx = k_xx*scale_x**2\n",
    "        k_yy = k_yy*scale_y**2\n",
    "        k_zz = k_zz*scale_z**2\n",
    "    \n",
    "    \n",
    "        #implement the PDE\n",
    "        D = tf.cast(tf.exp(self.D_var), tf.float32)\n",
    "        r = tf.cast(tf.exp(self.r_var), tf.float32)\n",
    "        u = k\n",
    "        u_x = k_x\n",
    "        u_y = k_y\n",
    "        u_z = k_z\n",
    "        u_xx = k_xx\n",
    "        u_yy = k_yy\n",
    "        u_zz = k_zz\n",
    "        #change it to float32\n",
    "        u_xx = tf.cast(u_xx, tf.float32)\n",
    "        u_yy = tf.cast(u_yy, tf.float32)\n",
    "        u_zz = tf.cast(u_zz, tf.float32)\n",
    "        u= tf.cast(u, tf.float32)\n",
    "        #u_t is float32 do not match the u_xx\n",
    "        u_t = tf.cast(D*(u_xx+u_yy+u_zz) + r*u*(1.0-u) , tf.float32)\n",
    "\n",
    "        # condition for the initial condition\n",
    "        dt = (t[-1] - t[0])[0] #total time\n",
    "        U0_pred=k-dt*tf.matmul(u_t,tf.transpose(IRK_alpha))\n",
    "        U1_pred=k+dt*tf.matmul(u_t,tf.transpose(IRK_beta-IRK_alpha))\n",
    "        # Format exact solutions for loss\n",
    "        U0 = q*[outputs_t0_noise]\n",
    "        U0 = tf.stack(U0, axis=1)   \n",
    "        U0 = tf.cast(U0, tf.float32)\n",
    "        U1 = q*[outputs_t1_noise]\n",
    "        U1 = tf.stack(U1, axis=1)\n",
    "        U1 = tf.cast(U1, tf.float32)\n",
    "\n",
    "        # Calculate loss\n",
    "        MSE = tf.reduce_mean(tf.square(U0 - U0_pred)) + tf.reduce_mean(tf.square(U1 - U1_pred))\n",
    "        return MSE\n",
    "\n",
    "    \n",
    "    def call(self,pred):              #call the loss function\n",
    "        self.add_loss(self.loss(pred))    #add loss to the model\n",
    "        return pred       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "d6046061-7c9e-47d2-a108-7c3a2411eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_guess = np.log(D)\n",
    "r_guess = np.log(r)\n",
    "My_loss=PDEsolution(D_guess,r_guess)(Predictions)\n",
    "# Create trainable model with custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73e2edc-cd22-45c7-a243-03074c1e6834",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss = tf.keras.Model(inputs=Inputs, outputs=My_loss)\n",
    "model_loss.compile(optimizer=tf.keras.optimizers.Adam())\n",
    "# Execute Adam optimization\n",
    "inputs_var=tf.convert_to_tensor(inputs_var)\n",
    "history=model_loss.fit(inputs_var,None, epochs=Adam_Epochs,verbose=0)  #None means no label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:opence-v1.5.1]",
   "language": "python",
   "name": "conda-env-opence-v1.5.1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
